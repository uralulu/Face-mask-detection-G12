import torch
import torchvision.transforms as transforms
from torchvision.models import efficientnet_b0
import gradio as gr
from PIL import Image
import numpy as np
import mediapipe as mp
import cv2 # C·∫ßn th∆∞ vi·ªán n√†y ƒë·ªÉ v·∫Ω l√™n ·∫£nh

# Load model
model = efficientnet_b0(weights=None)
model.classifier[1] = torch.nn.Linear(1280, 3)
model.load_state_dict(torch.load("best_efficientnet_b0.pth", map_location=torch.device("cpu")))
model.eval()

# Classes
class_names = ["mask_weared_incorrect", "with_mask", "without_mask"]

# Preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Mediapipe face detection (kh·ªüi t·∫°o m·ªôt l·∫ßn v√† t√°i s·ª≠ d·ª•ng)
face_detection_model = mp.solutions.face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)

# --- H√†m d·ª± ƒëo√°n ch√≠nh (s·∫Ω d√πng cho c·∫£ webcam v√† upload) ---
# H√†m n√†y c·∫ßn ph·∫£i ho·∫°t ƒë·ªông linh ho·∫°t cho c·∫£ live stream v√† ·∫£nh tƒ©nh
def predict(image_pil): # H√†m predict s·∫Ω nh·∫≠n m·ªôt PIL Image
    if image_pil is None:
        print("‚ö†Ô∏è Image is None (from Gradio)")
        return np.zeros((480, 640, 3), dtype=np.uint8), {name: 0.0 for name in class_names}

    image_np = np.array(image_pil) # Chuy·ªÉn PIL Image sang NumPy array (RGB)
    annotated_image = image_np.copy() # T·∫°o b·∫£n sao ƒë·ªÉ v·∫Ω

    try: # Th√™m kh·ªëi try-except ƒë·ªÉ b·∫Øt l·ªói
        # S·ª≠ d·ª•ng ƒë·ªëi t∆∞·ª£ng face_detection_model ƒë√£ kh·ªüi t·∫°o global
        results = face_detection_model.process(image_np)

        # N·∫øu kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t
        if not results.detections:
            cv2.putText(annotated_image, "No Face Detected", (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)
            return annotated_image, {"No face detected": 1.0}

        # L·∫∑p qua c√°c khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán (·ªü ƒë√¢y ch·ªâ x·ª≠ l√Ω c√°i ƒë·∫ßu ti√™n ƒë·ªÉ ƒë∆°n gi·∫£n)
        detection = results.detections[0]
        bboxC = detection.location_data.relative_bounding_box
        ih, iw, _ = image_np.shape # Chi·ªÅu cao v√† chi·ªÅu r·ªông c·ªßa ·∫£nh g·ªëc
        x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \
                     int(bboxC.width * iw), int(bboxC.height * ih)

        # ƒê·∫£m b·∫£o t·ªça ƒë·ªô bounding box n·∫±m trong gi·ªõi h·∫°n ·∫£nh
        x1, y1 = max(0, x), max(0, y)
        x2, y2 = min(iw, x + w), min(ih, y + h)

        # C·∫Øt v√πng khu√¥n m·∫∑t t·ª´ ·∫£nh PIL ban ƒë·∫ßu (tr√°nh c√°c v·∫•n ƒë·ªÅ v·ªÅ ƒë·ªãnh d·∫°ng m√†u)
        face_image_cropped_pil = image_pil.crop((x1, y1, x2, y2))

        # Ki·ªÉm tra n·∫øu ·∫£nh c·∫Øt ra b·ªã r·ªóng (v√≠ d·ª•, bounding box qu√° nh·ªè)
        if face_image_cropped_pil.size[0] == 0 or face_image_cropped_pil.size[1] == 0:
            cv2.putText(annotated_image, "Invalid Face Crop", (50, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)
            return annotated_image, {"Invalid face crop": 1.0}

        input_tensor = transform(face_image_cropped_pil).unsqueeze(0)

        with torch.no_grad():
            outputs = model(input_tensor)
            probs = torch.softmax(outputs, dim=1).numpy()[0]

        # L·∫•y nh√£n d·ª± ƒëo√°n v√† x√°c su·∫•t
        predicted_class_idx = np.argmax(probs)
        predicted_label = class_names[predicted_class_idx]
        confidence = probs[predicted_class_idx]

        # Ch·ªçn m√†u cho khung v√† text
        color = (0, 255, 0) # Green (RGB) for "with_mask"
        if predicted_label == "without_mask":
            color = (255, 0, 0) # Red (RGB) for "without_mask"
        elif predicted_label == "mask_weared_incorrect":
            color = (0, 165, 255) # Orange (RGB) for "mask_weared_incorrect"

        # V·∫Ω bounding box v√† text l√™n ·∫£nh
        cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2)
        cv2.putText(annotated_image, f"{predicted_label} ({confidence:.2f})", (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)

        return annotated_image, {class_names[i]: float(probs[i]) for i in range(len(class_names))}

    except Exception as e:
        print(f"üî¥ Error in predict: {e}")
        cv2.putText(annotated_image, f"Error: {str(e)[:50]}", (50, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)
        return annotated_image, {"Processing Error": 1.0}

# --- ƒê·ªãnh nghƒ©a Gr.Interface cho Webcam (GI·ªÆ NGUY√äN) ---
webcam_interface = gr.Interface(
    fn=predict,
    inputs=gr.Image(type="pil", sources=["webcam"], streaming=True, label="Webcam Feed"),
    outputs=[
        gr.Image(type="numpy", label="Detected Face with Prediction"),
        gr.Label(num_top_classes=3, label="Prediction Probability")
    ],
    live=True,
    title="Webcam Live Stream", # Title ri√™ng cho interface n√†y
    description="Live face mask detection from webcam.",
    allow_flagging="never"
)

# --- B·∫Øt ƒë·∫ßu c·∫•u tr√∫c Gr.Blocks ƒë·ªÉ ch·ª©a Tabs ---
custom_css = """
.output_image, .input_image {height: 400px !important; min-height: 200px !important;}
.gr-box {min-height: 400px !important;}
"""

with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("## Face Mask Detection (EfficientNetB0 + MediaPipe)")
    gr.Markdown("Detects face mask status using PyTorch and EfficientNetB0. Choose a tab below for **Webcam Live Stream** or **Upload/Paste Image**.")
    gr.Markdown("S·∫£n ph·∫©m demo c·ªßa nh√≥m 12 m√¥n AI, s·ª≠ d·ª•ng m√¥ h√¨nh EfficientNet (Image Classification).")
    with gr.Tabs():
        # --- TAB 1: Webcam Live Stream (Nh√∫ng gr.Interface ƒë√£ ƒë·ªãnh nghƒ©a) ---
        with gr.TabItem("Webcam Live Stream"):
            webcam_interface.render() # ƒê√¢y l√† c√°ch ƒë·ªÉ nh√∫ng gr.Interface v√†o gr.Blocks

        # --- TAB 2: Upload/Paste Image ---
        with gr.TabItem("Upload/Paste Image"):
            gr.Markdown("### Upload an image or paste from clipboard for prediction.")
            with gr.Row():
                upload_image_input = gr.Image(type="pil", sources=["upload", "clipboard"], label="Upload or Paste Image")
                upload_output_image = gr.Image(type="numpy", label="Detected Face with Prediction")
            upload_output_label = gr.Label(num_top_classes=3, label="Prediction Probability")
            
            # N√∫t Submit cho tab n√†y
            upload_button = gr.Button("Submit Image")

            # Khi n√∫t submit ƒë∆∞·ª£c nh·∫•n, g·ªçi h√†m predict cho ·∫£nh ƒë√£ upload/paste
            upload_button.click(
                fn=predict, # H√†m predict s·∫Ω x·ª≠ l√Ω ·∫£nh tƒ©nh
                inputs=upload_image_input,
                outputs=[upload_output_image, upload_output_label],
                api_name="upload_predict" # T√πy ch·ªçn: ƒë·∫∑t t√™n API
            )
            
            # T√πy ch·ªçn: K√≠ch ho·∫°t d·ª± ƒëo√°n khi ·∫£nh ƒë∆∞·ª£c t·∫£i l√™n (kh√¥ng c·∫ßn n√∫t Submit)
            # upload_image_input.change(
            #     fn=predict,
            #     inputs=upload_image_input,
            #     outputs=[upload_output_image, upload_output_label]
            # )

# Ch·∫°y ·ª©ng d·ª•ng Gradio
demo.launch(share=True, show_error=True)